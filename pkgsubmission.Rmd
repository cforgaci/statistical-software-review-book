
# Guide for Editors  {#pkgsubmission}

## Automated Checks

Upon initial submission, the `ropensci-review-bot` performs a suite of tests
and checks, and will paste a report into the GitHub issue thread. This report
is the primary source of information used to inform initial editorial
decisions. The best way to understand how decisions are to be made in response
to these reports is to provide a concrete example. The remainder of this
initial sub-section contains the contents of such a report, generated for
an R package generated by the [`srr::srr_stats_pkg_skeleton()`
function](https://ropensci-review-tools.github.io/srr/reference/srr_stats_pkg_skeleton.html),
with additional statistical standards inserted with the
[`srr::srr_stats_roxygen()`
function](https://ropensci-review-tools.github.io/srr/reference/srr_stats_roxygen.html).

All editors must familiarise themselves with the structure and contents of
these automated reports, an example of which can be seen by [clicking
here](https://github.com/ropenscilabs/statistical-software-review-book/blob/main/scripts/pkgcheck-ex.md).
A similar report can be reproduced locally by running the code within the following sub-section.
<details>
<summary>Package report code here (click to see).</summary>
<p>

```{r pkgcheck-code, eval = FALSE, echo = TRUE}
library (srr)
library (pkgcheck)
path <- srr_stats_pkg_skeleton ()
srr_stats_roxygen (category = "regression",
                   filename = file.path (path, "R", "srr-standards.R"))
check <- pkgcheck (path)
md <- checks_to_markdown (check, render = TRUE)
```

</p>
</details>



## Editor-in-Chief

As for rOpenSci's current peer-review system, packages are submitted directly
to the [`ropensci/software-review`
repository](https://github.com/ropensci/software-review) on GitHub, with
submissions handled initially by the rotating Editor-in-Chief (EiC).
Statistical software is nevertheless handled differently to standard
(non-statistical) packages from the first moment of submission. Statistical
submissions use a different template, the submission
of which automatically generates a detailed report from our
`ropensci-review-bot`, as described in the initial sub-section of this chapter,
and [illustrated
here](https://github.com/ropenscilabs/statistical-software-review-book/blob/main/scripts/pkgcheck-ex.md).

The EiC need only purvey the summary checks within the initial section of the
report. Most submissions should receive ticks for all items listed, and no
crosses, in which case the initial checklist will conclude with a statement that,

> *This package may be submitted*

In response to that statement, the sole tasks for an EiC prior to delegating
a handling editor are to check the following single item:

- [ ] The categories nominated by the submitting authors are appropriate for
  this package

And to choose one the following two items:

- [ ] The package does not fit within any additional categories of statistical
  software.
- [ ] The package could potentially be described by the following additional
  categories of statistical software:
    - \<list categories here\>


Additional effort by the EiC will only be required in "edge cases" where
a package may be unable to pass one of the checks, as in the [sample automated
check](https://github.com/ropenscilabs/statistical-software-review-book/blob/main/scripts/pkgcheck-ex.md),
which concludes with the statement that,

> *All failing checks above must be addressed prior to proceeding*

In these cases, submitting authors must explain why these checks may fail, and
the EiC must then determine whether these failures are acceptable.
Such cases ought nevertheless be rare, and it may be expected in the majority
of cases that the sole tasks of the EiC are to confirm a positive bot response,
to complete the two checklist items given above, and to allocate a handling
editor. This latter step is done by calling `@ropensci-review-bot assign <name>
as editor`.



## Handling Editor {#pkgsub-handling-editor}

The Handling Editor can use the summary report generated by the opening of the
issue (and [exemplified
here](https://github.com/ropenscilabs/statistical-software-review-book/blob/main/scripts/pkgcheck-ex.md))
to guide the steps towards assigning reviewers. While the EiC need only
consider the initial summary checklist, handling editors should consider all
details contained within the automated report.

The first section describes checks conducted by the [`srr` (**S**oftware
**R**eview **R**oclets) package](https://github.com/ropensci-review-tools/srr).
This check confirms that all statistical standards have been documented within
the code, and all packages must pass this check. The report linked to in that
section is primarily intended to aid reviews, and may be ignored by handling
editors.

The second section describes the "*Statistical Properties*" of the package
being submitted, and should be considered by handling editors. In particular,
this section contains information which identifies any statistically noteworthy
properties of the package. The example report illustrates how this report
immediately identifies that the package has very little code, very few
functions, and very few tests. Handling editors should consider these
statistical details, and particularly any noteworthy aspects (defined by
default as lying within upper or lower fifth percentiles in comparison with all
current CRAN packages). Any aspects which seem concerning should be explicitly
raised with submitting authors prior to proceeding. The measures currently
considered include various metrics for:

- Size of code base, both overall and in sub-directories
- Numbers of files in various sub-directories
- Numbers of functions
- Numbers of documentation lines per function
- Numbers of parameters per function
- Numbers of blank lines

A final metric, `fn_call_network_size`, quantifies the number of
inter-relationships between different functions. In `R` directories, these are
function calls, while relationships may be more complex within `src` or `inst`
directories. Small network sizes indicate packages which either construct few
objects (functions), or in which internal objects have no direct relationships.

The third and final section of the automated report contains details of
[`goodpractice`](https://github.com/MangoTheCat/goodpractice) checks including:

- Code coverage estimates for each file (from the [`covr`
  package](https://github.com/r-lib/covr)).
- Code style reports from the [`lintr`
  package](https://github.com/jimhester/lintr).
- Cyclomatic complexity reports from the [`cyclocomp`
  package](https://github.com/MangoTheCat/cyclocomp).
- Any errors, warnings, or notes raised by running `R CMD check` (from the
  [`rcmdcheck` package](https://github.com/r-lib/rcmdcheck).

Any aspects of these
[`goodpractice`](https://github.com/MangoTheCat/goodpractice) reports which do
not pass the initial checklist (such as warnings or errors from `R CMD check`,
or test coverage < 75%) should be clarified with authors prior to proceeding
with review.

Finally, the initial *Statistical Description* includes details of computer
languages used in a package, and should be used to ensure reviewers have
appropriate experience and abilities with the language(s) in which a package is
written.

### Re-generating package check results

The handling editor may update the initial package check results at any time
with the following command:

```
@ropensci-review-bot check package
```

This is likely to be necessary following each review, to ensure any issues
identified from the initial checks have been satisfactorily addressed.


### Handling Editor Checklist

Having thoroughly considered the automated package report, and addressed all
issues raised within that report, Handling Editors should paste the following
checklist items into the issue, and ensure that all items are able to be
checked prior to proceeding to delegate reviewers.

- [ ] Any issues raised during initial processing by EiC have been resolved (or
  there were none).
- [ ] **Either** (i) there were no issues raised in the `goodpractice` checks
  which need to be addressed prior to review, or (ii) all issues have been
  appropriately addressed.
- [ ] Running
  [`autotest_package()`](https://ropensci-review-tools.github.io/autotest/reference/autotest_package.html)
  on a local version of the source code generates no errors.
- [ ] **Either** (i) this package is aiming for a bronze badge, **or**, (ii)
  for packages aiming for silver or gold badges, the authors have clarified
  which of the four aspects listed in the "*Guide for Authors*" section on
  [silver badges](#pkgdev-silver) they intend to fulfil.
- [ ] **Either** (i) the package manifests no statistical anomalies in relation
  to the CRAN archive, **or** (ii) any statistical anomalies have been
  satisfactorily explained.

All of these items must be checked prior to assigning reviewers. This may
require iteration with submitting authors. As stated in the 
[*Dev Guide*](https://devguide.ropensci.org/editorguide.html#upon-submission):

- If authors believe changes might take time, [apply the holding
  label](#policiesreviewprocess) to the submission.
- If the package raises a new issue for rOpenSci policy, start a conversation
  in Slack or open a discussion on the [rOpenSci
  forum](https://discuss.ropensci.org/) to discuss it with other editors
  ([example of policy
  discussion](https://discuss.ropensci.org/t/overlap-policy-for-package-onboarding/368)).

Once all items have been checked, Handling Editors may assign reviewers using
the command `@ropensci-review-bot assign reviewer <name>`, and generally
following the procedure given in the [*Dev
Guide*](https://devguide.ropensci.org/editorguide.html#look-for-and-assign-reviewers).

### Disagreements on Badge Grades

Handling editors are responsible for resolving any disagreements between
authors' stated or desired grade of badge and reviewers' recommendations. The
views of reviewers should generally be prioritized in such cases. Grades as
declared by authors are contained in the opening comment of the issue. These
may be extracted by calling:

```
@ropensci-review-bot stats grade
```

Editors may modify these grades by editing the opening comment, and changing
the value of the `"statsgrade"` variable.

### Approval

After having completed a checklist and ensuring agreement on badge grade, the
handling editor may approve a submission with the following command:

```
@ropensci-review-bot approve
```

The bot will identify that this is a statistical software issue, extract the
appropriate grade, and attach a corresponding badge which will also label the
latest version of our statistical standards.
