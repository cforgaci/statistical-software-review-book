
# Guide for Reviewers {#pkgreview}

The current chapter should be considered an extension of the corresponding
["Guide for Reviewers"](https://devguide.ropensci.org/reviewerguide.html) in
rOpenSci's "Dev Guide". The principles for reviewing packages described there
also apply to statistical packages, with this chapter describing additional
processes and practices for review of packages submitted to the statistical
software peer review system. Being a direct extension, the [template for
general software review](https://devguide.ropensci.org/reviewtemplate.html) is
to be used allowing with the following additional considerations and components
to be entered into a review.

## Package badging


This system for peer-review of statistical software features badges in three
categories of **standard**, **silver**, and **gold**. As described in the
corresponding chapter for [package developers](#pkgdev-badges), these are:

1. A **standard** or default badge for software which has been accepted, and
   which reviewers have deemed sufficiently compliant with relevant standards.
3. A **silver** badge for software for which developers aspire to reach gold
   standard, but which is not yet sufficiently compliant. See below for
   details.
2. A **gold** badge for software which complies with *all* standards which
   reviewers have deemed potentially applicable. This is the highest standard,
   and requires developers to ensure compliance with the largest number of
   standards, while minimising the number of standards deemed non-applicable.

Developers will state which badge they hope to attain in a [*Life Cycle
Statement* within their `CONTRIBUTING.md` file](#pkgdev-badges). All reviewers
should complete the follow procedures for review of packages aiming for
a **standard** badge. Packages aiming for **silver** or **gold** badges should
then be reviewed according to the additional criteria described in the
subsequent sub-sections.

## General Review Requirements

The primary way that reviews of statistical software differ from reviews of
software submitted to rOpenSci's general peer-review system is that statistical
software is expected to conform to lists of general and category-specific
standards. The first important task of reviews is thus to assess the compliance
of software with these standards. The standards themselves are contained in the
[subsequent chapter](#standards), to which both developers and reviewers will
need to refer throughout package development and review. For software to be
considered within scope, it must be described by one or more of the categories
of statistical software for which standards have been developed. Software will
then need to comply both with the *General Standards* for statistical software,
and with specific sets of standards for each applicable category. These
categories will have been discussed and allocated in the pre-review phase, and
categorical applicability should generally not have to be considered during an
actual review.


### Assessment against standards

The process of assessing software against standards is facilitated by the
[`srr` (**s**oftware **r**eview **r**oclets)
package](https://github.com/ropenscilabs/srr) which both developers and
reviewers will need to install with the following line:

```{r srr-install-rev, eval = FALSE, echo = TRUE}
remotes::install_github("ropenscilabs/srr")
```

This package is primarily intended to aid developers in documenting both how
and where their software complies with each of the relevant general and
category-specific standards. Reviewers can then clone a local copy of the
repository to be reviewed, and in the root directory of that repository, run
the [`srr_report()`
function](https://ropenscilabs.github.io/srr/reference/srr_report.html)
to generate a hyperlinked `html` report of standards compliance.

Reviewers are requested to click on every single link which appears in that
report, and to at least briefly assess whether they believe the software at
each location complies with the nominated standards. The report itself is
divided into two main sections, named after the [roclet
tags](https://ropenscilabs.github.io/srr/articles/srr-stats.html#3-roxygen2-tags)
of:

1. `@srrstats` for standards with which software complies;
2. `@srrstatsNA` for standards which developers have deemed not to be
   applicable to their software.


Each of those two sections will then be divided into sub-sections according to
where within the repository those standards are reported (generally meaning
which sub-directory, such as `R/`, `tests`/, or elsewhere). No action need be
taken on standards with which reviewers agree, whether because software
complies and has a tag of `@srrstats`, or because a standard is not applicable
and has a tag of `@srrstatsNA`. Reviewers are only asked to note any standards
with which they disagree, primarily either because of:

1. Disagreement in standards compliance, where developers have used a tag of
   `@srrstats` but a reviewer judges either the explanation or associated code
   to be insufficient for compliance; or
2. Disagreement about non-applicability of a standard, where developers have
   used a tag of `@srrstatsNA`, but a reviewer believes that standard ought to
   apply to the software.

Please progress through the entire report generated by the [`srr_report()`
function](https://ropenscilabs.github.io/srr/reference/srr_report.html), and
note all instances of disagreement, generally grouped into the two categories
described above. Reports of potential disagreements in standards compliance
can be included directly within a review issue, or reviewers can opt to
initially work directly with developers to resolve issues directly within the
code itself, as described in the following sub-section.

Note that this initial assessment against standards is intended only to clarify
and resolve statements of compliance with which reviewers disagree, and in
particular should be conducted entirely independent of whether or not software
may be aspiring to silver or gold badges. Procedures for those latter cases are
conducted in subsequent review phases, as described below.


### General Package Review

Reviews of statistical software, should generally reflect the processes
established in rOpenSci's general software review system, for which the best
source of information is provided by [reviews
themselves](https://github.com/ropensci/software-review/issues), along with the
[*Guide for Reviewers*](https://devguide.ropensci.org/reviewerguide.html).
Beyond adherence to standards, we ask reviewers to explicitly consider the
following aspects of software corresponding to the sub-sections of the
[*General Standards for Statistical
Software*](#general-standards):

1. **Documentation**: Is the documentation sufficient to enable general use of
   the package? Do the various components of documentation support and clarify
   one another?
2. **Input Structures and Pre-Processing** Are the input structures adequate and
   general enough for the purposes of the package? Would or should it be
   possible to expand the range of admissible inputs? Do pre-processing steps
   ensure appropriate conversion of a range of inputs to a common form?
3. **Algorithms** How well are algorithms encoded? Are there any aspects which
   could be improved?
4. **Output Structures and Return Values** Are the output structures adequate and
   general enough for the purposes of the package? Do the output structures
   enable sufficient inter-operability with other packages, or could they be
   modified to better enhance inter-operability?
5. **Testing** Regardless of actual coverage of tests, are there any fundamental
   software operations which are not sufficiently expressed in tests? Is there
   are need for extended tests?
6. **Visualisation** (where appropriate)

Many of the category-specific sets of standards also follow these general
sub-categories, and reviewers are asked to consider both the general and
category-specific aspects of software in regard to each of these aspects. Note
that for statistical software the assessment of algorithmic quality is
particularly important, whether assessed in terms of appropriateness of
implementation, efficiency, uniqueness, or other qualitative aspects. This
project attempts to set standardised expectations of many aspects of
statistical software, yet core statistical algorithms generally remain too
diverse to be considered or compared in any standardised way. We thus request
all reviewers to explicitly review the quality of the core statistical
algorithms contained within a package. Most category-specific standards include
a central "*Algorithmic Standards*" component which can be used to frame broad
considerations for algorithmic quality. The [*General Standard*
**G1.1**](#general-standards) also requires all similar algorithms or
implementations to be documented within the software, so reviewers should also
have access to a list of comparable implementations.

### Review for Silver and Gold Badges

As described above, a silver badge is given to software which aims for a gold
badge, but which is not yet sufficiently compliant with all possible standards.
An important task for reviewers of software aiming for silver or gold badges is
thus to identify all general and category-specific standards that are
potentially applicable, yet not currently complied with. Not all standards may
be able to be applied to a given piece of software. For example, software
designed to accept sparse matrix inputs from the [`Matrix`
package](https://cran.r-project.org/web/packages/Matrix/index.html) will be
unable to conform with many of the standards for general rectangular input
forms. 

For packages aiming for silver or gold badges, reviewers with thus first need
to identify all standards which they deem *potentially* applicable; that is,
with which the software currently does not comply yet potentially could.
Packages aiming for gold badges at the end of review will need to comply with
all such standards to pass review. Reviewers of packages aiming for silver will
need to:

1. Judge whether they consider compliance sufficient for the package to be
   accepted; and
2. Negotiate with developers a proposed schedule for compliance with all
   remaining standards (generally within one year of initial acceptance).

Packages receiving a silver badge at the end of review will be expected to be
reviewed again according to the proposed schedule. This subsequent review will
simply need to determine whether all nominated standards have indeed been
complied with.
